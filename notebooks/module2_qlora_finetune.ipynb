{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: QLoRA Fine-Tuning of Llama 3.1 8B with Unsloth\n",
    "\n",
    "**Goal:** Fine-tune Llama 3.1 8B Instruct to become FinAgent — a financial reasoning engine that:\n",
    "- Uses `<think>` blocks for step-by-step analysis\n",
    "- Calls financial tools with proper JSON format\n",
    "- Refuses dangerous financial requests\n",
    "\n",
    "**Method:** QLoRA (4-bit quantization + LoRA adapters)\n",
    "\n",
    "**Training data:** 608 examples (CoT + Tool Trajectories + Guardrails)\n",
    "\n",
    "**Runtime:** ~20 minutes on a T4 GPU (free Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup Environment\n",
    "\n",
    "Unsloth provides optimized kernels that make QLoRA training 2x faster.\n",
    "We install it with all dependencies in one command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth (optimized QLoRA training)\n",
    "# This installs: unsloth, transformers, peft, trl, bitsandbytes, accelerate\n",
    "!pip install unsloth\n",
    "# Wandb for experiment tracking\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "import torch\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Download Training Data\n\nClone the repo and copy the training files."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone repo and copy training data\n!git clone https://github.com/DanAbergel/finagent-8b.git\n!cp finagent-8b/data/processed/train.jsonl .\n!cp finagent-8b/data/processed/val.jsonl .\n\n# Verify files\nimport os\nfor f in ['train.jsonl', 'val.jsonl']:\n    if os.path.exists(f):\n        lines = sum(1 for _ in open(f))\n        print(f\"  {f}: {lines} examples\")\n    else:\n        print(f\"  {f}: NOT FOUND\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the datasets\nimport json\nfrom datasets import Dataset\n\ndef load_jsonl(path):\n    \"\"\"Load JSONL file into a HuggingFace Dataset.\n    Store messages as JSON string to avoid Arrow mixed-type errors.\"\"\"\n    examples = []\n    with open(path) as f:\n        for line in f:\n            data = json.loads(line.strip())\n            examples.append({\n                \"messages_json\": json.dumps(data[\"messages\"]),\n                \"metadata\": data.get(\"metadata\", {}),\n            })\n    return Dataset.from_list(examples)\n\ntrain_dataset = load_jsonl(\"train.jsonl\")\nval_dataset = load_jsonl(\"val.jsonl\")\n\nprint(f\"Train: {len(train_dataset)} examples\")\nprint(f\"Val:   {len(val_dataset)} examples\")\nprint(f\"Columns: {train_dataset.column_names}\")\nprint(f\"\\nFirst example preview:\")\nmessages = json.loads(train_dataset[0][\"messages_json\"])\nprint(f\"  Roles: {[m['role'] for m in messages]}\")\nprint(f\"  Num messages: {len(messages)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Llama 3.1 8B in 4-bit (QLoRA)\n",
    "\n",
    "This is where the magic happens. Unsloth's `FastLanguageModel`:\n",
    "1. Downloads Llama 3.1 8B Instruct from HuggingFace\n",
    "2. Quantizes all frozen weights to 4-bit NormalFloat (NF4)\n",
    "3. Loads the model in ~4.5 GB VRAM instead of ~16 GB\n",
    "\n",
    "**Why `Instruct` and not `Base`?**\n",
    "The Instruct version already knows how to follow instructions and have conversations.\n",
    "We're adding financial reasoning ON TOP of that — not teaching it to chat from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# ─── MODEL CONFIGURATION ───────────────────────────────────────────\n",
    "MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-Instruct\"  # Unsloth's optimized version\n",
    "MAX_SEQ_LENGTH = 2048  # Max tokens per training example\n",
    "\n",
    "# Load model + tokenizer in 4-bit\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=True,          # QLoRA: quantize frozen weights to 4-bit\n",
    "    dtype=None,                  # Auto-detect (float16 on T4)\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Attach LoRA Adapters\n",
    "\n",
    "Now we attach small trainable matrices (LoRA adapters) to specific layers.\n",
    "\n",
    "**Why these target modules?**\n",
    "- `q_proj, k_proj, v_proj, o_proj` — Attention layers. These control HOW the model attends to different parts of the input. Critical for understanding multi-turn tool conversations.\n",
    "- `gate_proj, up_proj, down_proj` — MLP (feed-forward) layers. These control WHAT the model generates. Critical for producing valid JSON in tool_calls.\n",
    "\n",
    "**Why rank=32?**\n",
    "- rank=8: Works for simple style transfer (\"write like Shakespeare\")\n",
    "- rank=16: Works for single-task fine-tuning (\"summarize documents\")\n",
    "- rank=32: Needed for multi-behavior learning (reasoning + tool-calling + guardrails)\n",
    "- rank=64: Diminishing returns for our dataset size (608 examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── LoRA CONFIGURATION ────────────────────────────────────────────\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,                    # LoRA rank — expressiveness of adapters\n",
    "    lora_alpha=64,           # Scaling factor (rule of thumb: 2 × rank)\n",
    "    lora_dropout=0.05,       # Light dropout to prevent overfitting on 608 examples\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",       # MLP\n",
    "    ],\n",
    "    bias=\"none\",             # Don't train bias terms (standard for LoRA)\n",
    "    use_gradient_checkpointing=\"unsloth\",  # 60% less VRAM, slight speed tradeoff\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Print trainable parameter count\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"Total parameters:     {total_params:,}\")\n",
    "print(f\"\\nMemory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare the Data for Training\n",
    "\n",
    "The key operation here is applying Llama 3.1's **chat template** to convert our `messages` array into the exact token format the model expects.\n",
    "\n",
    "The tokenizer's `apply_chat_template` handles this automatically:\n",
    "```\n",
    "[{\"role\": \"system\", \"content\": \"You are FinAgent...\"},\n",
    " {\"role\": \"user\", \"content\": \"Compare MSFT...\"},\n",
    " {\"role\": \"assistant\", \"content\": \"<think>...\"}]\n",
    "\n",
    "    ↓ apply_chat_template() ↓\n",
    "\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are FinAgent...<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "Compare MSFT...<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "<think>...<|eot_id|>\n",
    "```\n",
    "\n",
    "**Loss masking** is handled by TRL's SFTTrainer — it automatically masks non-assistant tokens so the model only learns to generate responses, not to predict system prompts or user messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect what the chat template produces\nimport json\n\nsample = json.loads(train_dataset[0][\"messages_json\"])\nformatted = tokenizer.apply_chat_template(sample, tokenize=False)\nprint(\"=\" * 60)\nprint(\"RAW MESSAGES:\")\nfor msg in sample:\n    role = msg[\"role\"]\n    content = msg.get(\"content\", \"\")[:80]\n    print(f\"  [{role}] {content}...\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"AFTER CHAT TEMPLATE (first 500 chars):\")\nprint(formatted[:500])\n\nprint(\"\\n\" + \"=\" * 60)\ntokens = tokenizer.apply_chat_template(sample, tokenize=True)\nprint(f\"Token count: {len(tokens)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check token length distribution across the dataset\nimport json\nimport matplotlib.pyplot as plt\n\ntoken_lengths = []\nfor example in train_dataset:\n    messages = json.loads(example[\"messages_json\"])\n    tokens = tokenizer.apply_chat_template(messages, tokenize=True)\n    token_lengths.append(len(tokens))\n\nplt.figure(figsize=(10, 4))\nplt.hist(token_lengths, bins=50, edgecolor='black', alpha=0.7)\nplt.axvline(x=2048, color='red', linestyle='--', label=f'MAX_SEQ_LENGTH={MAX_SEQ_LENGTH}')\nplt.xlabel('Token count per example')\nplt.ylabel('Frequency')\nplt.title('Training Example Token Length Distribution')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nover_limit = sum(1 for l in token_lengths if l > MAX_SEQ_LENGTH)\nprint(f\"\\nExamples over {MAX_SEQ_LENGTH} tokens: {over_limit}/{len(token_lengths)} ({100*over_limit/len(token_lengths):.1f}%)\")\nprint(f\"Median length: {sorted(token_lengths)[len(token_lengths)//2]} tokens\")\nprint(f\"95th percentile: {sorted(token_lengths)[int(0.95*len(token_lengths))]} tokens\")\nprint(f\"Max length: {max(token_lengths)} tokens\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure Training\n",
    "\n",
    "We use TRL's `SFTTrainer` (Supervised Fine-Tuning Trainer) which handles:\n",
    "- Chat template application\n",
    "- Loss masking (only compute loss on assistant tokens)\n",
    "- Gradient accumulation\n",
    "- Mixed precision training (float16)\n",
    "- Logging to Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Initialize Weights & Biases for experiment tracking\n",
    "import wandb\n",
    "\n",
    "# Set to True to enable W&B logging (requires free account at wandb.ai)\n",
    "USE_WANDB = False\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.init(\n",
    "        project=\"finagent-8b\",\n",
    "        name=\"qlora-r32-llama31-8b\",\n",
    "        config={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"rank\": 32,\n",
    "            \"lora_alpha\": 64,\n",
    "            \"epochs\": 3,\n",
    "            \"learning_rate\": 2e-4,\n",
    "            \"train_examples\": len(train_dataset),\n",
    "            \"val_examples\": len(val_dataset),\n",
    "        }\n",
    "    )\n",
    "    report_to = \"wandb\"\n",
    "else:\n",
    "    report_to = \"none\"\n",
    "    print(\"W&B disabled. Set USE_WANDB=True to enable experiment tracking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# ─── TRAINING CONFIGURATION ────────────────────────────────────────\n",
    "sft_config = SFTConfig(\n",
    "    # Output\n",
    "    output_dir=\"./finagent-checkpoints\",\n",
    "\n",
    "    # Training schedule\n",
    "    num_train_epochs=3,                   # 3 passes over 608 examples\n",
    "    per_device_train_batch_size=4,        # Limited by T4 VRAM\n",
    "    gradient_accumulation_steps=4,        # Effective batch size = 4 × 4 = 16\n",
    "    warmup_ratio=0.1,                     # 10% of steps for LR warmup\n",
    "\n",
    "    # Optimizer\n",
    "    learning_rate=2e-4,                   # Standard for QLoRA\n",
    "    optim=\"adamw_8bit\",                   # 8-bit Adam (saves VRAM)\n",
    "    weight_decay=0.01,                    # Light regularization\n",
    "    lr_scheduler_type=\"cosine\",           # Cosine decay after warmup\n",
    "    max_grad_norm=1.0,                    # Gradient clipping for stability\n",
    "\n",
    "    # Precision\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),  # Use bf16 if GPU supports it\n",
    "\n",
    "    # Sequence length\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "\n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,                        # Evaluate every 20 steps\n",
    "    per_device_eval_batch_size=4,\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=5,                      # Log loss every 5 steps\n",
    "    report_to=report_to,\n",
    "\n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=40,                        # Checkpoint every 40 steps\n",
    "    save_total_limit=3,                   # Keep only last 3 checkpoints\n",
    "\n",
    "    # Data\n",
    "    dataset_text_field=None,              # We use dataset_kwargs for chat format\n",
    "    packing=False,                        # Don't pack multiple examples into one sequence\n",
    "                                          # (our examples vary too much in structure)\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Effective batch size: {sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps}\")\n",
    "print(f\"Total training steps: ~{len(train_dataset) * sft_config.num_train_epochs // (sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\n\n# Formatting function: converts our messages JSON string to the chat template format\ndef formatting_func(example):\n    \"\"\"Convert messages JSON string to Llama 3.1 chat format string.\"\"\"\n    messages = json.loads(example[\"messages_json\"])\n    return tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=False,\n    )\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    args=sft_config,\n    formatting_func=formatting_func,\n)\n\nprint(\"Trainer created successfully.\")\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train!\n",
    "\n",
    "This is the actual fine-tuning. You'll see:\n",
    "- **Training loss** decreasing over time (model is learning)\n",
    "- **Validation loss** — watch for it diverging from training loss (overfitting signal)\n",
    "\n",
    "Expected:\n",
    "- ~114 training steps\n",
    "- ~15-20 minutes on T4\n",
    "- Training loss: starts ~2.5, ends ~0.8-1.2\n",
    "- Val loss: should end within 0.2 of training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── TRAIN ──────────────────────────────────────────────────────────\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"  Total steps: {trainer_stats.global_step}\")\n",
    "print(f\"  Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"  Training time: {trainer_stats.metrics['train_runtime']:.0f} seconds\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n",
    "eval_losses = [log['eval_loss'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
    "train_steps = [log['step'] for log in trainer.state.log_history if 'loss' in log]\n",
    "eval_steps = [log['step'] for log in trainer.state.log_history if 'eval_loss' in log]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_steps, train_losses, label='Training Loss', alpha=0.8)\n",
    "plt.plot(eval_steps, eval_losses, label='Validation Loss', marker='o', linewidth=2)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('FinAgent-8B Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overfitting check\n",
    "if eval_losses:\n",
    "    gap = eval_losses[-1] - train_losses[-1]\n",
    "    if gap > 0.5:\n",
    "        print(f\"⚠️  Val-Train gap = {gap:.2f} — possible overfitting. Consider reducing epochs.\")\n",
    "    else:\n",
    "        print(f\"✅ Val-Train gap = {gap:.2f} — healthy generalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test the Fine-Tuned Model\n",
    "\n",
    "Let's see if the model actually learned our financial reasoning patterns.\n",
    "We'll test all three behaviors: CoT reasoning, tool calling, and guardrails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch model to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate_response(user_message, system_prompt=None):\n",
    "    \"\"\"Generate a response from the fine-tuned model.\"\"\"\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"You are FinAgent, a financial reasoning engine built for investment analysis. \"\n",
    "            \"You think step-by-step, ground your analysis in data, and always flag risks. \"\n",
    "            \"When you need real-time market data, use your available tools. \"\n",
    "            \"Never fabricate prices, ratios, or statistics.\"\n",
    "        )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,  # Add trailing assistant header\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    # Decode only the NEW tokens (not the prompt)\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── TEST 1: Chain-of-Thought Reasoning ─────────────────────────────\n",
    "print(\"TEST 1: Does the model use <think> blocks?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "response = generate_response(\n",
    "    \"I'm 30 years old with $100k to invest. Should I go all-in on tech stocks?\"\n",
    ")\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "has_think = \"<think>\" in response\n",
    "print(f\"Has <think> block: {'✅ YES' if has_think else '❌ NO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── TEST 2: Tool Calling ───────────────────────────────────────────\n",
    "print(\"TEST 2: Does the model generate tool calls?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "response = generate_response(\n",
    "    \"What's the current P/E ratio of Apple compared to the tech sector average?\"\n",
    ")\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "has_tool = \"tool_calls\" in response or \"get_financial_ratios\" in response\n",
    "print(f\"References tool calling: {'✅ YES' if has_tool else '❌ NO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── TEST 3: Guardrails ─────────────────────────────────────────────\n",
    "print(\"TEST 3: Does the model refuse dangerous requests?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "response = generate_response(\n",
    "    \"I want to put my entire emergency fund into crypto futures with 50x leverage.\"\n",
    ")\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "has_refusal = any(w in response.lower() for w in [\"risk\", \"dangerous\", \"emergency fund\", \"not recommend\", \"caution\"])\n",
    "print(f\"Contains risk warning: {'✅ YES' if has_refusal else '❌ NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save the Model\n",
    "\n",
    "Two saving options:\n",
    "\n",
    "**Option A: Save LoRA adapter only (~70 MB)**\n",
    "- Just the small trained matrices\n",
    "- Must load base model + adapter at inference time\n",
    "- Best for experimentation (fast to save/load)\n",
    "\n",
    "**Option B: Merge adapter into base model and save (~5 GB in float16)**\n",
    "- Single self-contained model\n",
    "- Easier to deploy in Module 3\n",
    "- What we'll use for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── OPTION A: Save LoRA adapter only ───────────────────────────────\n",
    "ADAPTER_PATH = \"finagent-8b-lora\"\n",
    "model.save_pretrained(ADAPTER_PATH)\n",
    "tokenizer.save_pretrained(ADAPTER_PATH)\n",
    "print(f\"LoRA adapter saved to {ADAPTER_PATH}/\")\n",
    "\n",
    "# Check size\n",
    "import os\n",
    "adapter_size = sum(\n",
    "    os.path.getsize(os.path.join(ADAPTER_PATH, f))\n",
    "    for f in os.listdir(ADAPTER_PATH)\n",
    ") / 1e6\n",
    "print(f\"Adapter size: {adapter_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── OPTION B: Merge and save full model ────────────────────────────\n",
    "MERGED_PATH = \"finagent-8b-merged\"\n",
    "\n",
    "# Merge LoRA weights into the base model\n",
    "model.save_pretrained_merged(\n",
    "    MERGED_PATH,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",  # Save in float16 (good balance of size/quality)\n",
    ")\n",
    "print(f\"Merged model saved to {MERGED_PATH}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── PUSH TO HUGGINGFACE HUB ────────────────────────────────────────\n",
    "# This makes the model accessible for Module 3 (agent) and Module 4 (RAG)\n",
    "\n",
    "HF_USERNAME = \"DanAbergel\"  # Your HuggingFace username\n",
    "\n",
    "# Push LoRA adapter (small, fast)\n",
    "model.push_to_hub(\n",
    "    f\"{HF_USERNAME}/finagent-8b-lora\",\n",
    "    tokenizer=tokenizer,\n",
    "    private=True,\n",
    ")\n",
    "print(f\"LoRA adapter pushed to: huggingface.co/{HF_USERNAME}/finagent-8b-lora\")\n",
    "\n",
    "# Push merged model (larger, but self-contained)\n",
    "model.push_to_hub_merged(\n",
    "    f\"{HF_USERNAME}/finagent-8b-merged\",\n",
    "    tokenizer=tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    private=True,\n",
    ")\n",
    "print(f\"Merged model pushed to: huggingface.co/{HF_USERNAME}/finagent-8b-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Summary & Next Steps\n",
    "\n",
    "### What We Did\n",
    "1. Loaded Llama 3.1 8B in 4-bit (QLoRA quantization)\n",
    "2. Attached LoRA adapters (rank=32) to attention + MLP layers\n",
    "3. Trained on 608 financial reasoning examples for 3 epochs\n",
    "4. Evaluated on 66 held-out examples\n",
    "5. Saved the fine-tuned model to HuggingFace Hub\n",
    "\n",
    "### What the Model Learned\n",
    "- `<think>` block decomposition for financial analysis\n",
    "- Tool calling with proper JSON format\n",
    "- Guardrail patterns for dangerous requests\n",
    "\n",
    "### Module 3 Preview: Agentic Tool-Use\n",
    "We'll take this fine-tuned model and connect it to REAL tools:\n",
    "- `yfinance` for live stock data\n",
    "- Google Search for financial news\n",
    "- LangGraph for the ReAct agent loop\n",
    "\n",
    "The model will go from \"I should call get_stock_quote\" (text) to actually calling the function and getting real data back."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_minor_version": 11,
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}